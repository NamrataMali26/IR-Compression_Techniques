{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings Compression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install argparse\n",
        "!pip install json\n",
        "!pip install codecs\n",
        "!pip install logging\n",
        "!pip install os\n",
        "!pip install warnings\n",
        "!pip install pqkmeans"
      ],
      "metadata": {
        "id": "zTeb9HJ9yaND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = '/content/drive/My Drive/IR/questions-words_trimmed.txt'\n",
        "model_path = '/content/drive/My Drive/IR/crawl-300d-50K.vec'"
      ],
      "metadata": {
        "id": "UadQmiK1yr0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import codecs\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.models import KeyedVectors\n",
        "import pqkmeans\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "def reduce_dimensions_pca(vectors, dimensions=150):\n",
        "    reduced_vectors = PCA(n_components=dimensions).fit_transform(vectors)\n",
        "    return reduced_vectors\n",
        "\n",
        "\n",
        "def product_quantize(vectors, subdims=30, centres=1000):\n",
        "    encoder = pqkmeans.encoder.PQEncoder(iteration=40, num_subdim=subdims, Ks=centres)\n",
        "    encoder.fit(vectors)\n",
        "    vectors_pq = encoder.transform(vectors)\n",
        "    reconstructed_vectors = encoder.inverse_transform(vectors_pq)\n",
        "    return reconstructed_vectors, vectors_pq, encoder.codewords\n",
        "\n",
        "\n",
        "def compute_accuracy(model):\n",
        "    print(\"Calculating accuracy...\")\n",
        "    accuracy, _ = model.evaluate_word_analogies('questions-words.txt', restrict_vocab=50000)\n",
        "    print(\"Accuracy: {:f}%\".format(accuracy*100))\n",
        "\n",
        "\n",
        "def save_matrix(file, matrix):\n",
        "    matrix_shape = list(matrix.shape)\n",
        "    matrix_list = matrix.flatten().tolist()\n",
        "    data = {\n",
        "        \"shape\": matrix_shape,\n",
        "        \"vectors\": matrix_list\n",
        "    }\n",
        "    with open(file, 'w') as f:\n",
        "        print(\"Saving {:s}\".format(file))\n",
        "        f.write(json.dumps(data))\n",
        "\n",
        "\n",
        "def human_format(num):\n",
        "    if not os.path.exists(\"generated\"):\n",
        "        os.makedirs(\"generated\")\n",
        "\n",
        "    magnitude = 0\n",
        "    while abs(num) >= 1000:\n",
        "        magnitude += 1\n",
        "        num /= 1000.0\n",
        "    # add more suffixes if you need them\n",
        "    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])\n",
        "\n",
        "\n",
        "def save_model(path, embedding_size, word_list, codes, centroids):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    model_name = \"embeddings-{}d-{}.vec\".format(embedding_size, human_format(len(word_list)))\n",
        "    model.save_word2vec_format(os.path.join(path, model_name))\n",
        "\n",
        "    vocab_file_path = os.path.join(path, \"vocab.json\")\n",
        "    with codecs.open(vocab_file_path, 'w', 'UTF-8') as f:\n",
        "        print(\"Saving {:s}\".format(vocab_file_path))\n",
        "        f.write(json.dumps(word_list))\n",
        "\n",
        "    save_matrix(os.path.join(path, 'codes.json'), codes)\n",
        "    save_matrix(os.path.join(path, 'centroids.json'), centroids)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5mwl7MBSR1Fb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaseLine accuracy in cell below"
      ],
      "metadata": {
        "id": "K2w-uE-Jy9m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def compute_accuracy(model):\n",
        "    accuracy, _ = model.evaluate_word_analogies(test_path, restrict_vocab=50000)\n",
        "    print(\"Accuracy: {:f}%\".format(accuracy*100))\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(model_path)\n",
        "# Compute baseline accuracy\n",
        "compute_accuracy(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL-0TqW2R8Ke",
        "outputId": "76195d63-b497-4ebe-ca02-e06e00d72e33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.433048%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA Dimensionality reduction in cell below"
      ],
      "metadata": {
        "id": "0SjZbVXJzFDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_dimensions_pca(vectors, dimensions=150):\n",
        "    reduced_vectors = PCA(n_components=dimensions).fit_transform(vectors)\n",
        "    return reduced_vectors\n",
        "\n",
        "original_embeddings = model.vectors\n",
        "reduced_embeddings = reduce_dimensions_pca(original_embeddings)\n",
        "\n",
        "# Create a new model with the reduced embeddings and calculate the accuracy\n",
        "words = [model.index2word[idx] for idx in range(len(reduced_embeddings))]\n",
        "model = KeyedVectors(vector_size=reduced_embeddings.shape[1])\n",
        "model.add(words, reduced_embeddings, replace=True)\n",
        "compute_accuracy(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKgNGjpMS03d",
        "outputId": "da0ca549-1d19-4a18-d9b1-98ca45a7bd88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.866097%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Product Quantization in cell below"
      ],
      "metadata": {
        "id": "W5KHmK23zF9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pqkmeans\n",
        "\n",
        "def product_quantize(vectors, subdims=100, centres=1000):\n",
        "    encoder = pqkmeans.encoder.PQEncoder(iteration=40, num_subdim=subdims, Ks=centres)\n",
        "    encoder.fit(vectors)\n",
        "    vectors_pq = encoder.transform(vectors)\n",
        "    reconstructed_vectors = encoder.inverse_transform(vectors_pq)\n",
        "    return reconstructed_vectors, vectors_pq, encoder.codewords\n",
        "\n",
        "reconstructed_embeddings, codes, centroids = product_quantize(original_embeddings)\n",
        "\n",
        "# Compute accuracy of new model\n",
        "words = [model.index2word[idx] for idx in range(len(reconstructed_embeddings))]\n",
        "model = KeyedVectors(vector_size=reconstructed_embeddings.shape[1])\n",
        "model.add(words, reconstructed_embeddings, replace=True)\n",
        "compute_accuracy(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZeToCajUOrq",
        "outputId": "d008684f-4d57-4ae8-ed03-38340bba808e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 97.720798%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size Reduction Code in Cell Below"
      ],
      "metadata": {
        "id": "ZCG3M6pozJ2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_size = original_embeddings.nbytes\n",
        "reduced_size =  reduced_embeddings.nbytes\n",
        "new_size = codes.nbytes + centroids.nbytes\n",
        "print(\"Size reduction: {:f}%\".format((original_size - new_size) * 100 / original_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWqn5O0vWQR-",
        "outputId": "b84a1f7d-ffc8-432e-d68c-3277e1c88235"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size reduction: 79.333333%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hd4Op39DbNPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}